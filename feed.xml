<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://chanhyeok-choi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://chanhyeok-choi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-26T08:54:27+00:00</updated><id>https://chanhyeok-choi.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">딥러닝 공부 자료 모음집</title><link href="https://chanhyeok-choi.github.io/blog/2024/kor/" rel="alternate" type="text/html" title="딥러닝 공부 자료 모음집"/><published>2024-12-06T15:01:13+00:00</published><updated>2024-12-06T15:01:13+00:00</updated><id>https://chanhyeok-choi.github.io/blog/2024/kor</id><content type="html" xml:base="https://chanhyeok-choi.github.io/blog/2024/kor/"><![CDATA[<h1 id="content">Content</h1> <h2 id="한국어-버전-korean">한국어 버전 (Korean)</h2> <ul> <li> <h3 id="deep-reinforcement-learning">Deep Reinforcement Learning</h3> <p>김성훈 교수님의 <a href="https://hunkim.github.io/ml/">모두의 딥러닝 사이트</a> 에서 <strong>Deep Reinforcement Learning</strong>을 들어보시는 걸 추천드립니다. 한국어 강의이며 강의 하나당 15분 내외로 하루만에 완강하실 수 있습니다. Q-learning이 무엇인지부터 시작해서 DQN이 나오게된 배경과 알고리즘 동작을 이해하기 쉽게 설명해주십니다.</p> <p>강화학습의 기초를 다지기에 도움이 많이 되었습니다.</p> </li> <li> <h3 id="auto-encoder">Auto Encoder</h3> <p>이활석 박사님의 <a href="https://www.youtube.com/watch?v=o_peo6U7IRM">오토인코더의 모든 것</a>은 생성 모델 중 하나인 <strong>Auto Encoder</strong>에 대해 3시간분의 강의를 3편으로 나눠서 해주십니다.</p> <p>강의 초반에 수학 수식이 다소 등장하기 때문에 딥러닝과 생성모델에 대해 어느정도 공부하신 분들께서 들어보시길 추천드리고, 전체적인 흐름을 잡는데 도움이 많이 되었습니다.</p> </li> </ul> <h2 id="영어-버전-english">영어 버전 (English)</h2> <ul> <li> <h3 id="deep-unsupervised-learning">Deep Unsupervised Learning</h3> <p>UC Berkeley의 Pieter Abbeel 교수님께서 강의하시는 Deep Unsupervised Learning (<a href="https://sites.google.com/view/berkeley-cs294-158-sp24/home">course website</a>)을 수강해보시는 걸 권유드립니다. <a href="https://www.youtube.com/watch?v=tFR6Likf4VI&amp;list=PLwRJQ4m4UJjPIvv4kgBkvu_uygrV3ut_U&amp;index=2">유튜브 강의 링크</a>에서 보실 수 있으며 생성 모델 전반을 다루고 최신 모델 동향까지 파악할 수 있는 보석같은 강의입니다.</p> </li> <li> <h3 id="deep-learning-in-computer-vision">Deep Learning in Computer Vision</h3> <p>정말 유명한 스탠퍼드 강의입니다. <a href="https://cs231n.stanford.edu/">cs231n</a>에서 모든 자료들을 확인하실 수 있습니다. 컴퓨터 비전에 딥러닝을 적용하는 과정을 담고 있으며 그 흐름 속에서 CV에 대한 통찰력을 기르실 수 있습니다. 실습 과제가 오픈되어 있으니 코랩에서 코딩까지 해보시는 걸 추천드립니다.</p> </li> </ul>]]></content><author><name></name></author><category term="DL"/><category term="deep-learning,"/><category term="VAE"/><summary type="html"><![CDATA[딥러닝 공부를 하면서 도움이 되었던 자료들을 모두 모았습니다.]]></summary></entry><entry><title type="html">Multimodal interaction in talking face generation and analysis in MCR</title><link href="https://chanhyeok-choi.github.io/blog/2024/eng/" rel="alternate" type="text/html" title="Multimodal interaction in talking face generation and analysis in MCR"/><published>2024-12-03T21:44:13+00:00</published><updated>2024-12-03T21:44:13+00:00</updated><id>https://chanhyeok-choi.github.io/blog/2024/eng</id><content type="html" xml:base="https://chanhyeok-choi.github.io/blog/2024/eng/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Along with the development of deep learning, the AI industry has attracted a lot of investment and enabled advancement of various applications such as ChatGPT, Gemini and SegTalker <a href="#1">[1]</a>. SegTalker, which is a model that can generate a talking face video based on a single image and audio file, contributes to success of broader applications including digital reporter and video dubbing by modeling different modalities adequately. However, although it can capture lip synchronization well and enable local editing with improvement on overall texture, current generative techniques [<a href="#5">5,</a> <a href="#6">6,</a> <a href="#7">7,</a> <a href="#8">8,</a> <a href="#9">9,</a> <a href="#12">12</a>] face challenges in reflecting complex and interactive scenarios. For example, it is impossible to generate a video about a reporter getting wet under the rain or snow, frowning and explaining about the current weather at the same time. So, it is necessary to solve this issue for realizing virtual reality and avatar by knowledge fusion in modality space [<a href="#2">2,</a> <a href="#3">3,</a> <a href="#4">4</a>].</p> <h2 id="analysis">Analysis</h2> <p>In recent years, there has been a lot of effort in talking face generation, typically focusing on synthesizing photo-realistic video and synchronizing lip movements with audio [<a href="#5">5,</a> <a href="#6">6,</a> <a href="#7">7,</a> <a href="#8">8,</a> <a href="#9">9,</a> <a href="#22">22,</a> <a href="#23">23,</a> <a href="#24">24</a>]. However, they have still the limitation requiring a massive training dataset to satisfy somewhat of performance. Furthermore, they do not demonstrate the capability of complicated interactions such as generating a talking avatar surprised under thunder lightning. To address this problem, we can leverage large and pretrained multimodal models [<a href="#10">10,</a> <a href="#11">11</a>] and combine their ability to expand representations of model, inheriting the superiors and enhancing capacity without additional dataset by C-MCR method <a href="#2">[2]</a>. In fact, if expanding the knowledge of model, it can achieve higher performance on audio-visual, audio-text, visual-text retrieval tasks than existing works <a href="#3">[3]</a>.</p> <p>In these methods, the authors have effectively encoded different modalities into a semantically aligned shared space by establishing inter- and intra- Multimodal Contrastive Representations (MCR) connections <a href="#2">[2]</a> or aligning multiple existing MCRs into the same based MCR <a href="#3">[3]</a>. Meanwhile, they are specifically designed for one and only one shared modality, which restricts the utility. Hence, to mitigate the issue, it needs to consider extra modality connections. Therefore, it seems indispensable to leverage displacement and combination bond of different modal encoders, improving multimodal understanding of unified model <a href="#4">[4]</a>.</p> <p>FreeBind <a href="#4">[4]</a> surpasses the advanced audio-text and image-text expert spaces but cannot capture the temporal consistency of data. In fact, the existing methods in knowledge fusion do not have the capacity to handle video dataset. However, most of prior works in talking face generation have considered the temporal awareness of generative model as handling sequences <a href="#12">[12]</a> or intermediates [<a href="#6">6,</a> <a href="#7">7,</a> <a href="#9">9</a>], which is essential in the task. Due to this reason, it seems inevitable to inject temporal attention layers into a main model for video training <a href="#16">[16]</a>. However, it needs to investigate another methodology transferring condition (e.g., getting wet under the rain) into frames appropriately because we would like to utilize pretrained models without additional training. AdaIN <a href="#13">[13]</a> operation, which is one of the representatives in style transfer on deep learning, would probably resolve the concern. Furthermore, the method of adjusting cross-attention layers between other models has been studied to reflect a style image into a ground image flexibly <a href="#15">[15]</a> and ControlNet <a href="#14">[14]</a> suggests how to fine-tune a model with an extra condition efficiently and effectively.</p> <p>To the best of our knowledge, there is no standard methodology to deal with complex interactions in talking face generation task either with training or without training. Even if video editing techniques have advanced in the most versatile generation model including GAN <a href="#19">[19]</a> and Diffusion <a href="#20">[20]</a>, it is not trivial to edit talking face video due to difficulty in lip synchronization, texture quality, and preserving identity of avatar after reflecting additional conditions. So, we suggest the method of injecting copied layers, where adjusting conditions, into a segmentation-based diffusion model by leveraging knowledge fusion encoders for conditional generation [<a href="#1">1,</a> <a href="#4">4,</a> <a href="#17">17,</a> <a href="#18">18</a>]. Then, it would be able to flexibly project multimodal prompts into our model and maintain lip and head movements adequately.</p> <h2 id="conclusion">Conclusion</h2> <p>A lot of AI related applications and technologies have been developed these days. Especially, researchers have investigated methods in talking face generation, focusing on preserving identity, lip sync and video realistic. In recent, the segmentation-based method demonstrates capability to edit talking face video. Moreover, many experiments in coherence to multimodal prompts have been also conducted and outperformed in zero-shot situations, which implies that we can leverage these techniques for complicated interactions to generative model. Furthermore, the image-to-video editing technique such as diffusion inversion [<a href="#18">18,</a> <a href="#21">21</a>] enables a model to be aware of conditional prompts (e.g., text or audio). Therefore, it will be a valid attempt to insert temporal attention layers with unified multimodal encoders and apply the diffusion inversion at inference step for reflecting complex scenarios and interactions without additional dataset and training in talking face generation. If it is successful, then it can accelerate the development of applications such as editable digital avatar and virtual animation. In the future, we might see a virtual reporter talking about regional weather, in which his or her hair is fluttering when raining and wind blowing.</p> <h2 id="references">References</h2> <p><a id="1">[1]</a> Xiong, Lingyu, et al. “SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing.” <em>Proceedings of the 32nd ACM International Conference on Multimedia.</em> 2024.</p> <p><a id="2">[2]</a> Wang, Zehan, et al. “Connecting multi-modal contrastive representations.” <em>Advances in Neural Information Processing Systems 36</em> (2023): 22099-22114.</p> <p><a id="3">[3]</a> Wang, Zehan, et al. “Extending multi-modal contrastive representations.” <em>arXiv preprint arXiv:2310.08884</em> (2023).</p> <p><a id="4">[4]</a> Wang, Zehan, et al. “FreeBind: Free Lunch in Unified Multimodal Space via Knowledge Fusion.” In <em>Forty-first International Conference on Machine Learning.</em></p> <p><a id="5">[5]</a> Shen, Shuai, et al. “Difftalk: Crafting diffusion models for generalized audio-driven portraits animation.” <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.</em> 2023.</p> <p><a id="6">[6]</a> Zhou, Yang, et al. “Makelttalk: speaker-aware talking-head animation.” <em>ACM Transactions On Graphics (TOG) 39.6</em> (2020): 1-15.</p> <p><a id="7">[7]</a> Zhong, Weizhi, et al. “Identity-preserving talking face generation with landmark and appearance priors.” <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.</em> 2023.</p> <p><a id="8">[8]</a> Prajwal, K. R., et al. “A lip sync expert is all you need for speech to lip generation in the wild.” <em>Proceedings of the 28th ACM international conference on multimedia.</em> 2020.</p> <p><a id="9">[9]</a> Zhang, Wenxuan, et al. “Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation.” <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.</em> 2023.</p> <p><a id="10">[10]</a> Radford, Alec, et al. “Learning transferable visual models from natural language supervision.” <em>International conference on machine learning. PMLR,</em> 2021.</p> <p><a id="11">[11]</a> Wu, Yusong, et al. “Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation.” <em>ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,</em> 2023.</p> <p><a id="12">[12]</a> Xu, Sicheng, et al. “Vasa-1: Lifelike audio-driven talking faces generated in real time.” <em>arXiv preprint arXiv:2404.10667</em> (2024).</p> <p><a id="13">[13]</a> Huang, Xun, and Serge Belongie. “Arbitrary style transfer in real-time with adaptive instance normalization.” <em>Proceedings of the IEEE international conference on computer vision.</em> 2017.</p> <p><a id="14">[14]</a> Zhang, Lvmin, Anyi Rao, and Maneesh Agrawala. “Adding conditional control to text-to-image diffusion models.” <em>Proceedings of the IEEE/CVF International Conference on Computer Vision.</em> 2023.</p> <p><a id="15">[15]</a> Chen, Chun-Fu Richard, Quanfu Fan, and Rameswar Panda. “Crossvit: Cross-attention multi-scale vision transformer for image classification.” <em>Proceedings of the IEEE/CVF international conference on computer vision.</em> 2021.</p> <p><a id="16">[16]</a> Chen, Zhiyuan, et al. “Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions.” <em>arXiv preprint arXiv:2407.08136</em> (2024).</p> <p><a id="17">[17]</a> Guo, Yuwei, et al. “Animatediff: Animate your personalized text-to-image diffusion models without specific tuning.” <em>arXiv preprint arXiv:2307.04725</em> (2023).</p> <p><a id="18">[18]</a> Mokady, Ron, et al. “Null-text inversion for editing real images using guided diffusion models.” <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.</em> 2023.</p> <p><a id="19">[19]</a> Goodfellow, Ian, et al. “Generative adversarial networks.” <em>Communications of the ACM 63.11</em> (2020): 139-144.</p> <p><a id="20">[20]</a> Ho, Jonathan, Ajay Jain, and Pieter Abbeel. “Denoising diffusion probabilistic models.” <em>Advances in neural information processing systems 33</em> (2020): 6840-6851.</p> <p><a id="21">[21]</a> Ceylan, Duygu, Chun-Hao P. Huang, and Niloy J. Mitra. “Pix2video: Video editing using image diffusion.” <em>Proceedings of the IEEE/CVF International Conference on Computer Vision.</em> 2023.</p> <p><a id="22">[22]</a> Guo, Yudong, et al. “Ad-nerf: Audio driven neural radiance fields for talking head synthesis.” <em>Proceedings of the IEEE/CVF international conference on computer vision.</em> 2021.</p> <p><a id="23">[23]</a> Suwajanakorn, Supasorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman. “Synthesizing obama: learning lip sync from audio.” <em>ACM Transactions on Graphics (ToG) 36.4</em> (2017): 1-13.</p> <p><a id="24">[24]</a> Shen, Shuai, et al. “Learning dynamic facial radiance fields for few-shot talking head synthesis.” <em>European conference on computer vision.</em> Cham: Springer Nature Switzerland, 2022.</p>]]></content><author><name></name></author><category term="multimodal"/><category term="talking-face-generation,"/><category term="MCR,"/><category term="interaction"/><summary type="html"><![CDATA[Insight about multimodal space beyond talking face generation]]></summary></entry></feed>