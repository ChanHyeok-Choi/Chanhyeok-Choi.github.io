<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://chanhyeok-choi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://chanhyeok-choi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-25T01:07:00+00:00</updated><id>https://chanhyeok-choi.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Collection of Deep Learning Study Resources</title><link href="https://chanhyeok-choi.github.io/blog/2024/kor/" rel="alternate" type="text/html" title="Collection of Deep Learning Study Resources"/><published>2024-12-06T15:01:13+00:00</published><updated>2024-12-06T15:01:13+00:00</updated><id>https://chanhyeok-choi.github.io/blog/2024/kor</id><content type="html" xml:base="https://chanhyeok-choi.github.io/blog/2024/kor/"><![CDATA[<h1 id="content">Content</h1> <h2 id="korean-version">Korean Version</h2> <ul> <li> <h3 id="deep-reinforcement-learning">Deep Reinforcement Learning</h3> <p>I highly recommend the <strong>Deep Reinforcement Learning</strong> lectures by Prof. Sung Kim, available on the<br/> <a href="https://hunkim.github.io/ml/">Everyone’s Deep Learning</a> website.<br/> The lectures are in Korean, each about 15 minutes long, and can be completed in a single day.<br/> The series starts from the basics of Q-learning and builds up to DQN, explaining the motivation and algorithmic workflow in a very intuitive way.</p> <p>These lectures were extremely helpful for building a solid foundation in reinforcement learning.</p> </li> <li> <h3 id="auto-encoder">Auto Encoder</h3> <p>Dr. Hwal Seok Lee’s video series, <em>Everything About Autoencoders</em><br/> (<a href="https://www.youtube.com/watch?v=o_peo6U7IRM">YouTube link</a>), provides a detailed, 3-part introduction to <strong>Autoencoders</strong>, one of the fundamental generative models.</p> <p>Since the lectures include some mathematical formulations early on, I recommend them to those who already have basic knowledge of deep learning and generative models. They were very helpful in understanding the overall structure and intuition behind autoencoders.</p> </li> </ul> <h2 id="english-version">English Version</h2> <ul> <li> <h3 id="deep-unsupervised-learning">Deep Unsupervised Learning</h3> <p>I strongly recommend UC Berkeley Prof. Pieter Abbeel’s<br/> <strong>Deep Unsupervised Learning</strong> course<br/> (<a href="https://sites.google.com/view/berkeley-cs294-158-sp24/home">course website</a>).<br/> You can watch the lectures via this<br/> <a href="https://www.youtube.com/watch?v=tFR6Likf4VI&amp;list=PLwRJQ4m4UJjPIvv4kgBkvu_uygrV3ut_U&amp;index=2">YouTube playlist</a>.<br/> It is a gem of a course that covers a wide range of generative models and provides insights into recent trends in the field.</p> </li> <li> <h3 id="deep-learning-in-computer-vision">Deep Learning in Computer Vision</h3> <p>A widely acclaimed course from Stanford University,<br/> <strong>CS231n: Convolutional Neural Networks for Visual Recognition</strong>, offers comprehensive material on applying deep learning to computer vision.<br/> All resources are available on the official website:<br/> <a href="https://cs231n.stanford.edu/">https://cs231n.stanford.edu/</a><br/> The course provides excellent intuition and practical understanding of CV workflows. Since the programming assignments are publicly available, I highly recommend trying them out in Colab for hands-on experience.</p> </li> </ul>]]></content><author><name></name></author><category term="DL"/><category term="deep-learning,"/><category term="VAE"/><summary type="html"><![CDATA[A curated list of materials that were particularly helpful while studying deep learning.]]></summary></entry><entry><title type="html">Multimodal interaction in talking face generation and analysis in MCR</title><link href="https://chanhyeok-choi.github.io/blog/2024/eng/" rel="alternate" type="text/html" title="Multimodal interaction in talking face generation and analysis in MCR"/><published>2024-12-03T21:44:13+00:00</published><updated>2024-12-03T21:44:13+00:00</updated><id>https://chanhyeok-choi.github.io/blog/2024/eng</id><content type="html" xml:base="https://chanhyeok-choi.github.io/blog/2024/eng/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Along with the development of deep learning, the AI industry has attracted a lot of investment and enabled advancement of various applications such as ChatGPT, Gemini and SegTalker <a href="#1">[1]</a>. SegTalker, which is a model that can generate a talking face video based on a single image and audio file, contributes to success of broader applications including digital reporter and video dubbing by modeling different modalities adequately. However, although it can capture lip synchronization well and enable local editing with improvement on overall texture, current generative techniques [<a href="#5">5,</a> <a href="#6">6,</a> <a href="#7">7,</a> <a href="#8">8,</a> <a href="#9">9,</a> <a href="#12">12</a>] face challenges in reflecting complex and interactive scenarios. For example, it is impossible to generate a video about a reporter getting wet under the rain or snow, frowning and explaining about the current weather at the same time. So, it is necessary to solve this issue for realizing virtual reality and avatar by knowledge fusion in modality space [<a href="#2">2,</a> <a href="#3">3,</a> <a href="#4">4</a>].</p> <h2 id="analysis">Analysis</h2> <p>In recent years, there has been a lot of effort in talking face generation, typically focusing on synthesizing photo-realistic video and synchronizing lip movements with audio [<a href="#5">5,</a> <a href="#6">6,</a> <a href="#7">7,</a> <a href="#8">8,</a> <a href="#9">9,</a> <a href="#22">22,</a> <a href="#23">23,</a> <a href="#24">24</a>]. However, they have still the limitation requiring a massive training dataset to satisfy somewhat of performance. Furthermore, they do not demonstrate the capability of complicated interactions such as generating a talking avatar surprised under thunder lightning. To address this problem, we can leverage large and pretrained multimodal models [<a href="#10">10,</a> <a href="#11">11</a>] and combine their ability to expand representations of model, inheriting the superiors and enhancing capacity without additional dataset by C-MCR method <a href="#2">[2]</a>. In fact, if expanding the knowledge of model, it can achieve higher performance on audio-visual, audio-text, visual-text retrieval tasks than existing works <a href="#3">[3]</a>.</p> <p>In these methods, the authors have effectively encoded different modalities into a semantically aligned shared space by establishing inter- and intra- Multimodal Contrastive Representations (MCR) connections <a href="#2">[2]</a> or aligning multiple existing MCRs into the same based MCR <a href="#3">[3]</a>. Meanwhile, they are specifically designed for one and only one shared modality, which restricts the utility. Hence, to mitigate the issue, it needs to consider extra modality connections. Therefore, it seems indispensable to leverage displacement and combination bond of different modal encoders, improving multimodal understanding of unified model <a href="#4">[4]</a>.</p> <p>FreeBind <a href="#4">[4]</a> surpasses the advanced audio-text and image-text expert spaces but cannot capture the temporal consistency of data. In fact, the existing methods in knowledge fusion do not have the capacity to handle video dataset. However, most of prior works in talking face generation have considered the temporal awareness of generative model as handling sequences <a href="#12">[12]</a> or intermediates [<a href="#6">6,</a> <a href="#7">7,</a> <a href="#9">9</a>], which is essential in the task. Due to this reason, it seems inevitable to inject temporal attention layers into a main model for video training <a href="#16">[16]</a>. However, it needs to investigate another methodology transferring condition (e.g., getting wet under the rain) into frames appropriately because we would like to utilize pretrained models without additional training. AdaIN <a href="#13">[13]</a> operation, which is one of the representatives in style transfer on deep learning, would probably resolve the concern. Furthermore, the method of adjusting cross-attention layers between other models has been studied to reflect a style image into a ground image flexibly <a href="#15">[15]</a> and ControlNet <a href="#14">[14]</a> suggests how to fine-tune a model with an extra condition efficiently and effectively.</p> <p>To the best of our knowledge, there is no standard methodology to deal with complex interactions in talking face generation task either with training or without training. Even if video editing techniques have advanced in the most versatile generation model including GAN <a href="#19">[19]</a> and Diffusion <a href="#20">[20]</a>, it is not trivial to edit talking face video due to difficulty in lip synchronization, texture quality, and preserving identity of avatar after reflecting additional conditions. So, we suggest the method of injecting copied layers, where adjusting conditions, into a segmentation-based diffusion model by leveraging knowledge fusion encoders for conditional generation [<a href="#1">1,</a> <a href="#4">4,</a> <a href="#17">17,</a> <a href="#18">18</a>]. Then, it would be able to flexibly project multimodal prompts into our model and maintain lip and head movements adequately.</p> <h2 id="conclusion">Conclusion</h2> <p>A lot of AI related applications and technologies have been developed these days. Especially, researchers have investigated methods in talking face generation, focusing on preserving identity, lip sync and video realistic. In recent, the segmentation-based method demonstrates capability to edit talking face video. Moreover, many experiments in coherence to multimodal prompts have been also conducted and outperformed in zero-shot situations, which implies that we can leverage these techniques for complicated interactions to generative model. Furthermore, the image-to-video editing technique such as diffusion inversion [<a href="#18">18,</a> <a href="#21">21</a>] enables a model to be aware of conditional prompts (e.g., text or audio). Therefore, it will be a valid attempt to insert temporal attention layers with unified multimodal encoders and apply the diffusion inversion at inference step for reflecting complex scenarios and interactions without additional dataset and training in talking face generation. If it is successful, then it can accelerate the development of applications such as editable digital avatar and virtual animation. In the future, we might see a virtual reporter talking about regional weather, in which his or her hair is fluttering when raining and wind blowing.</p> <h2 id="references">References</h2> <p><a id="1">[1]</a> Xiong, Lingyu, et al. “SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing.” <em>Proceedings of the 32nd ACM International Conference on Multimedia.</em> 2024.</p> <p><a id="2">[2]</a> Wang, Zehan, et al. “Connecting multi-modal contrastive representations.” <em>Advances in Neural Information Processing Systems 36</em> (2023): 22099-22114.</p> <p><a id="3">[3]</a> Wang, Zehan, et al. “Extending multi-modal contrastive representations.” <em>arXiv preprint arXiv:2310.08884</em> (2023).</p> <p><a id="4">[4]</a> Wang, Zehan, et al. “FreeBind: Free Lunch in Unified Multimodal Space via Knowledge Fusion.” In <em>Forty-first International Conference on Machine Learning.</em></p> <p><a id="5">[5]</a> Shen, Shuai, et al. “Difftalk: Crafting diffusion models for generalized audio-driven portraits animation.” <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.</em> 2023.</p> <p><a id="6">[6]</a> Zhou, Yang, et al. “Makelttalk: speaker-aware talking-head animation.” <em>ACM Transactions On Graphics (TOG) 39.6</em> (2020): 1-15.</p> <p><a id="7">[7]</a> Zhong, Weizhi, et al. “Identity-preserving talking face generation with landmark and appearance priors.” <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.</em> 2023.</p> <p><a id="8">[8]</a> Prajwal, K. R., et al. “A lip sync expert is all you need for speech to lip generation in the wild.” <em>Proceedings of the 28th ACM international conference on multimedia.</em> 2020.</p> <p><a id="9">[9]</a> Zhang, Wenxuan, et al. “Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation.” <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.</em> 2023.</p> <p><a id="10">[10]</a> Radford, Alec, et al. “Learning transferable visual models from natural language supervision.” <em>International conference on machine learning. PMLR,</em> 2021.</p> <p><a id="11">[11]</a> Wu, Yusong, et al. “Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation.” <em>ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,</em> 2023.</p> <p><a id="12">[12]</a> Xu, Sicheng, et al. “Vasa-1: Lifelike audio-driven talking faces generated in real time.” <em>arXiv preprint arXiv:2404.10667</em> (2024).</p> <p><a id="13">[13]</a> Huang, Xun, and Serge Belongie. “Arbitrary style transfer in real-time with adaptive instance normalization.” <em>Proceedings of the IEEE international conference on computer vision.</em> 2017.</p> <p><a id="14">[14]</a> Zhang, Lvmin, Anyi Rao, and Maneesh Agrawala. “Adding conditional control to text-to-image diffusion models.” <em>Proceedings of the IEEE/CVF International Conference on Computer Vision.</em> 2023.</p> <p><a id="15">[15]</a> Chen, Chun-Fu Richard, Quanfu Fan, and Rameswar Panda. “Crossvit: Cross-attention multi-scale vision transformer for image classification.” <em>Proceedings of the IEEE/CVF international conference on computer vision.</em> 2021.</p> <p><a id="16">[16]</a> Chen, Zhiyuan, et al. “Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions.” <em>arXiv preprint arXiv:2407.08136</em> (2024).</p> <p><a id="17">[17]</a> Guo, Yuwei, et al. “Animatediff: Animate your personalized text-to-image diffusion models without specific tuning.” <em>arXiv preprint arXiv:2307.04725</em> (2023).</p> <p><a id="18">[18]</a> Mokady, Ron, et al. “Null-text inversion for editing real images using guided diffusion models.” <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.</em> 2023.</p> <p><a id="19">[19]</a> Goodfellow, Ian, et al. “Generative adversarial networks.” <em>Communications of the ACM 63.11</em> (2020): 139-144.</p> <p><a id="20">[20]</a> Ho, Jonathan, Ajay Jain, and Pieter Abbeel. “Denoising diffusion probabilistic models.” <em>Advances in neural information processing systems 33</em> (2020): 6840-6851.</p> <p><a id="21">[21]</a> Ceylan, Duygu, Chun-Hao P. Huang, and Niloy J. Mitra. “Pix2video: Video editing using image diffusion.” <em>Proceedings of the IEEE/CVF International Conference on Computer Vision.</em> 2023.</p> <p><a id="22">[22]</a> Guo, Yudong, et al. “Ad-nerf: Audio driven neural radiance fields for talking head synthesis.” <em>Proceedings of the IEEE/CVF international conference on computer vision.</em> 2021.</p> <p><a id="23">[23]</a> Suwajanakorn, Supasorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman. “Synthesizing obama: learning lip sync from audio.” <em>ACM Transactions on Graphics (ToG) 36.4</em> (2017): 1-13.</p> <p><a id="24">[24]</a> Shen, Shuai, et al. “Learning dynamic facial radiance fields for few-shot talking head synthesis.” <em>European conference on computer vision.</em> Cham: Springer Nature Switzerland, 2022.</p>]]></content><author><name></name></author><category term="multimodal"/><category term="talking-face-generation,"/><category term="MCR,"/><category term="interaction"/><summary type="html"><![CDATA[Insight about multimodal space beyond talking face generation]]></summary></entry></feed>